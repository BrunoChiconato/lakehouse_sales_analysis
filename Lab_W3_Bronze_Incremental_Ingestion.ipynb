{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7a363f-c60a-4d2c-8abf-0c9282d21787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Environment and Paths\n",
    "Define key variables for the catalog, schemas, and volume paths. This step centralizes configuration, making the notebook easier to manage and adapt. We also set the active catalog and schema to simplify subsequent commands by avoiding fully qualified names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff53e5e-63ac-4f66-b6eb-3848e7cbe6d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_catalog = \"main\"\n",
    "\n",
    "volume_schema = \"lakehouse_sales\"\n",
    "volume_name = \"raw\"\n",
    "\n",
    "bronze_schema = \"lakehouse_sales_bronze\"\n",
    "bronze_table = \"bronze_sales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154114ba-a9bd-4ff8-b8a9-86ea07ee551c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {main_catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {volume_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b131aab-e8b0-4f38-b52d-5821efba13aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Synthetic Sales Data\n",
    "To make this notebook self-contained, we'll generate a realistic dataset of sales orders. A Python function using the **Faker** library creates mock data, including order IDs, dates, countries, and product details. This simulates a typical raw data source without requiring external file dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8efaece-6a07-4dfb-bde5-d1a9a61bbcd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "def generate_orders(start_id=1, end_id=100, start_date='2024-01-01', end_date='2024-12-31', seed=42):\n",
    "    \"\"\"\n",
    "    Generates a list of dictionaries in the format:\n",
    "    {\"order_id\": int, \"order_date\": \"YYYY-MM-DD\", \"country\": \"BR\", \"sku\": \"SKU-001\", \"qty\": int, \"unit_price\": float}\n",
    "    \"\"\"\n",
    "    Faker.seed(seed)\n",
    "    random.seed(seed)\n",
    "    fake = Faker()\n",
    "\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "    data = []\n",
    "    for order_id in range(start_id, end_id + 1):\n",
    "        order_date = fake.date_between(start_date=start, end_date=end).strftime('%Y-%m-%d')\n",
    "        country = fake.country_code()\n",
    "        sku = f\"SKU-{random.randint(1, 50):03d}\"\n",
    "        qty = random.randint(1, 5)\n",
    "        unit_price = round(random.uniform(9.0, 499.0), 2)\n",
    "\n",
    "        data.append({\n",
    "            \"order_id\": order_id,\n",
    "            \"order_date\": order_date,\n",
    "            \"country\": country,\n",
    "            \"sku\": sku,\n",
    "            \"qty\": qty,\n",
    "            \"unit_price\": unit_price\n",
    "        })\n",
    "    return data\n",
    "\n",
    "data = generate_orders(start_id=1000, end_id=2000, start_date='2024-01-01', end_date='2024-12-31', seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37092984-8eb9-4aa2-a8fe-f0d2ff865e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stage Raw Data in a Unity Catalog Volume\n",
    "The generated data is converted into a Spark DataFrame and then written as a CSV file to a pre-configured Unity Catalog Volume. This pattern mimics a common real-world scenario where raw data files land in a staging location before being loaded into the Bronze layer of the lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a3d640-42c2-4bb1-83fd-a6b7c7a7929c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "volume_path = f\"/Volumes/{main_catalog}/{volume_schema}/{volume_name}\"\n",
    "csv_path = f\"{volume_path}/raw_sales_2.csv\"\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.coalesce(1).write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b971603c-b6fd-48c4-8167-38e597e5bfca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingest Raw Data into the Bronze Table\n",
    "Use the `COPY INTO` command to idempotently load the raw sales data from the CSV file into the `bronze_sales` Delta table. This command is highly efficient for incremental data ingestion. We use `mergeSchema` to allow for schema evolution, which accommodates new columns in the source data without failing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1108fa7e-991b-4de8-8217-dc2a9d4ac3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE SCHEMA {bronze_schema}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {bronze_table}\n",
    "    FROM '/Volumes/{main_catalog}/{volume_schema}/{volume_name}/raw_sales_2.csv'\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS('header'='true', 'inferSchema'='true')\n",
    "    COPY_OPTIONS('mergeSchema'='true')\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c0cde85-6ee7-4801-af20-f8df1cff4565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validate Bronze Table Ingestion\n",
    "Run simple validation queries to confirm the success of the ingestion process.\n",
    "- The first query counts the total rows to ensure that data was loaded into the `bronze_sales` table.\n",
    "- The second query checks the table's transaction history, providing metadata about the `COPY INTO` operation and confirming the versioning capabilities of Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787c83fc-31f9-4647-ba5f-78e7ac717a96",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757800603515}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS bronze_count FROM bronze_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8ebe53-6903-493a-880f-08d6918a74c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY bronze_sales;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "faker"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5464323490001392,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab_W3_Bronze_Incremental_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
