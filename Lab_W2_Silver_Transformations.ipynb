{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c2d7f1-16b8-4e73-903b-016184a72d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Dependencies\n",
    "Import the necessary functions and classes from PySpark, including `pyspark.sql.functions` for transformations and `Window` for deduplication logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ab01c7-562a-4957-87ef-eab98e3c2181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac39487c-8439-4cb9-8664-05fd5701bab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Catalog and Schema\n",
    "Set the active catalog and schema for the current session. This ensures that all subsequent table references will resolve to our target `lakehouse_sales_bronze` schema without needing to fully qualify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9858f7f-6732-4e97-8b45-51ca61225cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lakehouse_catalog = \"main\"\n",
    "bronze_schema = \"lakehouse_sales_bronze\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {lakehouse_catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {bronze_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a1d258-e9a8-4547-86dc-518f0bd88555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Bronze Layer Data\n",
    "Load the raw `bronze_sales` table into a Spark DataFrame. We'll inspect its schema to confirm the initial data types before applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b834f8fe-e44f-4bf8-a2e8-28dbb656daae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_sales = spark.table(\"bronze_sales\")\n",
    "bronze_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0299d022-0827-426e-aed9-401cfa11e4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply Schema and Type Casting\n",
    "Define the Silver layer schema by selecting and transforming columns from the Bronze DataFrame. This step involves:\n",
    "- Casting columns to their appropriate data types (e.g., `long` for IDs, `date` for dates).\n",
    "- Adding a new `ingestion_ts` column to track when the record was processed into the Silver layer.\n",
    "Using `select` centralizes schema definition, making it cleaner and more performant than chained `withColumn` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1371842-beea-4b3c-aeca-c9e4d6e77d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = bronze_sales.select(\n",
    "    F.col(\"order_id\").cast(\"long\"),\n",
    "    F.to_date(F.col(\"order_date\")).alias(\"order_date\"),\n",
    "    F.col(\"country\").cast(\"string\"),\n",
    "    F.col(\"sku\").cast(\"string\"),\n",
    "    F.col(\"qty\").cast(\"integer\"),\n",
    "    F.col(\"unit_price\").cast(\"double\"),\n",
    "    F.current_timestamp().alias(\"ingestion_ts\")\n",
    ")\n",
    "\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856971c2-3d87-4725-bcea-30794e0491cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deduplicate and Handle Nulls\n",
    "\n",
    "Clean the dataset by removing duplicates and rows with missing critical information.\n",
    "\n",
    "- We use a window that partitions by `order_id` and sorts by `ingestion_ts` to identify the **most recent record** for each order via `row_number()`.\n",
    "- For ranking functions used in deduplication (e.g., `row_number`, `rank`, `dense_rank`), **we do not define `rowsBetween(...)` explicitly**. With `ORDER BY`, Spark/Databricks already applies (and requires) the **`ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`** frame. Specifying `UNBOUNDED FOLLOWING` conflicts with this requirement and causes a parsing error.\n",
    "- When you work with **window aggregate functions** that need to scan the entire partition (e.g., a \"global\" `last_value` for the partition), then it may be necessary to **explicitly define the frame** (e.g., from `UNBOUNDED PRECEDING` to `UNBOUNDED FOLLOWING`), depending on the use case.\n",
    "- Finally, we discard records with `null` in essential business columns (`order_id`, `order_date`, etc.) to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0263ff12-4be3-48ef-b57b-2c5069ef8a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window = (\n",
    "    Window\n",
    "    .partitionBy(\"order_id\")\n",
    "    .orderBy(F.col(\"ingestion_ts\").desc())\n",
    ")\n",
    "\n",
    "silver_deduped_df = (silver_df\n",
    "    .withColumn(\"rn\", F.row_number().over(window))\n",
    "    .filter(\"rn = 1\")\n",
    "    .drop(\"rn\")\n",
    "    .dropna(subset=[\"order_id\", \"order_date\", \"sku\", \"qty\", \"unit_price\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec54208-0b01-4c89-a39a-172952618803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Silver Layer\n",
    "Create the target Silver schema if it doesn't already exist and write the cleaned, deduplicated DataFrame as a managed Delta table named `silver_sales`.\n",
    "- `mode(\"overwrite\")` ensures the table is fully replaced during development runs.\n",
    "- `option(\"overwriteSchema\", \"true\")` allows schema evolution if we add or modify columns in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13492edf-6729-4400-b9b2-ffe575a69c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_schema = \"lakehouse_sales_silver\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {lakehouse_catalog}.{silver_schema}\")\n",
    "spark.sql(f\"USE SCHEMA {silver_schema}\")\n",
    "\n",
    "silver_sales = (\n",
    "    silver_deduped_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"silver_sales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9470a126-a670-4ef3-8a2d-22c8106c1a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validation Query: Total Row Count\n",
    "Run a simple SQL query to verify that the `silver_sales` table was created and populated with data. Counting the total rows is a basic sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f427d673-dfdf-4842-b3c6-2f12e04a4dfe",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"revenue\":202},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757021486713}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  COUNT(*) AS total_rows\n",
    "FROM\n",
    "  silver_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9d3e15-5796-4f18-a2ec-1b33f5679f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validation Query: Revenue by Country\n",
    "Execute a more complex aggregation to validate business logic. This query calculates total revenue per country, which helps confirm that numeric types (`qty`, `unit_price`) are correct and that aggregate functions perform as expected without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9710c6a-6a28-4d37-a3db-2388d8a85406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  country,\n",
    "  SUM(qty*unit_price) AS revenue\n",
    "FROM\n",
    "  silver_sales\n",
    "GROUP BY\n",
    "  country\n",
    "ORDER BY\n",
    "  revenue DESC;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4954691005620122,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab_W2_Silver_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
